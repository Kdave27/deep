{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f675704-36cd-4380-af48-3c8bf0f058d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer using minst\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))  # Reshape for CNN\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))\n",
    "X_train = X_train.astype('float32') / 255  # Normalize to [0, 1]\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create a simple CNN model\n",
    "def create_model(optimizer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Optimizers\n",
    "optimizers = {\n",
    "    'Batch Gradient Descent': 'sgd',  # Using SGD as a representation\n",
    "    'Stochastic Gradient Descent': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Momentum': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Adagrad': tf.keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "}\n",
    "\n",
    "# Train and evaluate models using different optimizers\n",
    "results = {}\n",
    "for name, optimizer in optimizers.items():\n",
    "    model = create_model(optimizer)\n",
    "    print(f\"\\nTraining with {name}...\")\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=2)\n",
    "    results[name] = history.history\n",
    "\n",
    "# Plot accuracy for each optimizer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in results.items():\n",
    "    plt.plot(history['val_accuracy'], label=name)\n",
    "    \n",
    "plt.title('Model Accuracy with Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss for each optimizer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in results.items():\n",
    "    plt.plot(history['val_loss'], label=name)\n",
    "\n",
    "plt.title('Model Loss with Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1567c3-0f22-453a-a9c2-19d25bccfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term (intercept)\n",
    "X_b = np.c_[np.ones((100, 1)), x]\n",
    "\n",
    "# Function to compute cost\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    return (1/m) * np.sum((X.dot(theta) - y) ** 2)\n",
    "\n",
    "# Batch Gradient Descent with cost tracking\n",
    "def batch_gradient_descent(X, y, learning_rate=0.1, iterations=1000):\n",
    "    m = len(X)\n",
    "    theta = np.random.randn(2, 1)  # Random initialization\n",
    "    for i in range(iterations):\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(X, y, learning_rate=0.1, n_epochs=50):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)  # Random initialization\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index + 1]\n",
    "            yi = y[random_index:random_index + 1]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Mini-batch Gradient Descent with cost tracking\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.1, n_iterations=50, batch_size=20):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)  # Random initialization\n",
    "    for iteration in range(n_iterations):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_b_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "            gradients = 2/len(xi) * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "# Momentum-based Gradient Descent\n",
    "def gradient_descent_momentum(X, y, learning_rate=0.01, iterations=1000, gamma=0.9):\n",
    "    m = len(y)\n",
    "    theta = np.zeros((X.shape[1], 1))  # Zero initialization\n",
    "    velocity = np.zeros_like(theta)\n",
    "    for i in range(iterations):\n",
    "        gradients = 1/m * X.T.dot(X.dot(theta) - y)\n",
    "        velocity = gamma * velocity - learning_rate * gradients\n",
    "        theta += velocity\n",
    "    return theta\n",
    "# Adagrad\n",
    "def adagrad(X, y, learning_rate=0.1, epsilon=1e-8, iterations=1000):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)  # Random initialization\n",
    "    gradient_accum = np.zeros_like(theta)\n",
    "    for i in range(iterations):\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        gradient_accum += gradients ** 2\n",
    "        adjusted_gradients = gradients / (np.sqrt(gradient_accum) + epsilon)\n",
    "        theta -= learning_rate * adjusted_gradients\n",
    "    return theta\n",
    "\n",
    "# Calculate thetas for basic optimizers\n",
    "theta_bgd = batch_gradient_descent(X_b, y)\n",
    "theta_sgd = stochastic_gradient_descent(X_b, y)\n",
    "theta_mbgd = mini_batch_gradient_descent(X_b, y)\n",
    "\n",
    "# Calculate thetas for advanced optimizers\n",
    "theta_momentum = gradient_descent_momentum(X_b, y)\n",
    "theta_adagrad = adagrad(X_b, y)\n",
    "\n",
    "# Plotting for Basic Optimizers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, X_b.dot(theta_bgd), color='red', linewidth=4, label='Batch GD (Broad Line)')  # Broader line for BGD\n",
    "plt.plot(x, X_b.dot(theta_sgd), color='green', linewidth=2, label='SGD')\n",
    "plt.plot(x, X_b.dot(theta_mbgd), color='purple', linewidth=2, label='Mini-batch GD')\n",
    "\n",
    "# Labels and legend for basic optimizers\n",
    "plt.title('Basic Gradient Descent Optimizers')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting for Advanced Optimizers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, X_b.dot(theta_momentum), color='orange', linewidth=2, label='Momentum GD')\n",
    "plt.plot(x, X_b.dot(theta_adagrad), color='brown', linewidth=2, label='Adagrad')\n",
    "\n",
    "# Labels and legend for advanced optimizers\n",
    "plt.title('Advanced Gradient Descent Optimizers')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
